{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: REWRITE to the latest RAW corpuses format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG = {'device': 0, 'cpu_count': 4}\n",
    "TESTS = False\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "def load_entries(fn): # '../data/datasets/nerus.jsonl.gz'\n",
    "    entries = []\n",
    "    with gzip.open(fn, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            entry = json.loads(line)\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "    #del entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.corpus import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NERUS_ds_test = load_entries('../../data/datasets/nerus_test.jsonl.gz')\n",
    "# NERUS_ds_train = load_entries('../../data/datasets/nerus_train.jsonl.gz')\n",
    "# NERUS = Corpus.from_raw('ru', NERUS_ds_train, NERUS_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    ents = {}\n",
    "    ds_train = []\n",
    "    ds_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476020a7b6d94fdca47f928225d07872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad45757a7ca24f3eaad9b8ee80e42fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NERUS = Corpus()\n",
    "NERUS.ents = {'ORG', 'PER', 'LOC'}\n",
    "NERUS.ds_test = load_entries('../../data/datasets/nerus_test.jsonl.gz')\n",
    "NERUS.ds_train = load_entries('../../data/datasets/nerus_train.jsonl.gz')\n",
    "\n",
    "# BUGFIX for v0.5\n",
    "# NERUS.ds_test = [{'raw': x['raw'], 'entities': x['entries']} for x in tqdm(NERUS.ds_test)]\n",
    "# NERUS.ds_train = [{'raw': x['raw'], 'entities': x['entries']} for x in tqdm(NERUS.ds_train)]\n",
    "\n",
    "# BUGFIX for v0.9\n",
    "NERUS.ner = NERUS.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA = [NERUS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 719295 20000\n"
     ]
    }
   ],
   "source": [
    "for c in CORPORA:\n",
    "    print(len(c.ner), len(c.ds_train), len(c.ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_pipes(nlp, x):\n",
    "    return [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluck(dict_list, attr):\n",
    "    r = []\n",
    "    for e in dict_list:\n",
    "        r.append(e[attr])\n",
    "    return r\n",
    "\n",
    "def pluck_list(dict_list, *attrs):\n",
    "    r = []\n",
    "    for e in dict_list:\n",
    "        r.append([e[a] for a in attrs])\n",
    "    return r\n",
    "\n",
    "def pluck_dict(dict_list, *attrs):\n",
    "    r = []\n",
    "    for e in dict_list:\n",
    "        r.append({a: e[a] for a in attrs})\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def setup_model():\n",
    "#     print(\"Creating new model...\")\n",
    "#     nlp = spacy.blank('ru')\n",
    "#     return nlp\n",
    "# nlp = setup_model()\n",
    "# from tokenizer import set_stemming_tokenizer\n",
    "# set_stemming_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f2abb223550>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f2abb1bf5e8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f2abb1bf648>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_RU2 = False\n",
    "if USE_RU2:\n",
    "    import ru2e\n",
    "    nlp = ru2e.load_ru2('../../ru2_pos_dep_stemming')\n",
    "else:\n",
    "    nlp = spacy.load('../../ru2')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new NER...\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f2abb223550>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f2abb1bf5e8>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f2abb1bf648>)]\n",
      "('PER', 'ORG', 'LOC')\n"
     ]
    }
   ],
   "source": [
    "def add_ner(nlp, labels, rebuild=True):\n",
    "    if 'ner' in nlp.pipe_names and rebuild:\n",
    "        nlp.disable_pipes('ner')\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        print(\"Creating new NER...\")\n",
    "        nlp_ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(nlp_ner)\n",
    "    else:\n",
    "        print(\"Using existing NER...\")\n",
    "        nlp_ner = nlp.get_pipe('ner')\n",
    "    print(nlp.pipeline)\n",
    "    for l in labels:\n",
    "        nlp_ner.add_label(l)\n",
    "    assert set(labels) <= set(nlp_ner.labels)\n",
    "    return nlp_ner\n",
    "nlp_ner = add_ner(nlp, [l for c in CORPORA for l in c.ents], rebuild=True)\n",
    "print(nlp_ner.labels)\n",
    "\n",
    "assert nlp.get_pipe('ner') == nlp_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tqdm import tqdm_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_names(nlp, unseen):\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    return [mt for i, mt in enumerate(ner.move_names) if i in unseen]\n",
    "    \n",
    "def enable_all_entities(nlp):\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    ner.model.unseen_classes = set()\n",
    "\n",
    "def enable_entities(nlp, labels):\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    # print(\"Unseen classes were:\", unseen_names(nlp, ner.model.unseen_classes))\n",
    "    unseen = set()\n",
    "    for i, mt in enumerate(ner.move_names):\n",
    "        if '-' in mt:\n",
    "            l = mt.split('-', 1)[1]\n",
    "            # print(mt, l)\n",
    "            if l not in labels:\n",
    "                unseen.add(i)\n",
    "    # print(\"Set unseen classes to:\", unseen_names(nlp, unseen))\n",
    "    ner.model.unseen_classes = unseen\n",
    "\n",
    "if TESTS:\n",
    "    print('Training', KR.ents)\n",
    "    enable_entities(nlp, KR.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "import pandas\n",
    "pandas.set_option('display.precision', 3) \n",
    "\n",
    "def _evaluate(model, batches):\n",
    "    if model.get_pipe('ner').model is True:\n",
    "        print(\"Initializing model!\")\n",
    "        model.begin_training(**CFG)\n",
    "    scorer = Scorer()\n",
    "    for batch in batches:\n",
    "        docs = pluck(batch, 'raw')\n",
    "        docs = model.pipe(docs)\n",
    "        for doc, parse in zip(docs, batch):\n",
    "            scorer.score(doc, GoldParse(doc, entities=parse['entities']))\n",
    "    return scorer.scores\n",
    "\n",
    "def evaluate(model, dataset, batch_size=32):\n",
    "    batches = tqdm_batches(minibatch(dataset, batch_size), total=len(dataset), leave=False)\n",
    "    return _evaluate(model, batches)\n",
    "\n",
    "def evaluate_data_source(model, ds, count=None, batch_size=32):\n",
    "    enable_entities(model, ds.ents)\n",
    "    if count:\n",
    "        dataset = random.sample(ds.ds_test, count)\n",
    "    else:\n",
    "        dataset = ds.ds_test\n",
    "    res = evaluate(model, dataset)\n",
    "    return {k:v for k,v in res['ents_per_type'].items() if k in ds.ents}\n",
    "\n",
    "def display_ents(list_of_scores):\n",
    "    display(pandas.DataFrame.from_records(list_of_scores).T)\n",
    "\n",
    "if TESTS or 0:\n",
    "    res = evaluate(nlp, NERUS.ds_test, 1000)\n",
    "    print(pluck_dict([res], 'token_acc', 'tags_acc', 'uas', 'las'))\n",
    "    display_ents(res['ents_per_type'])\n",
    "\n",
    "if TESTS or 0:\n",
    "    scores = evaluate_data_source(nlp, NERUS, count=1000, batch_size=32)\n",
    "    display_ents(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def _train_epoch(model, labels, batches):\n",
    "    with model.disable_pipes(*get_other_pipes(nlp, 'ner')):\n",
    "        if model.get_pipe('ner').model is True:\n",
    "            print(\"Initializing model!\")\n",
    "            model.begin_training(**CFG)\n",
    "        optimizer = model.resume_training(**CFG)\n",
    "        losses = {}\n",
    "        n_docs = 0\n",
    "        for batch in batches:\n",
    "            texts = pluck(batch, 'raw')\n",
    "            anns = pluck_dict(batch, 'entities')\n",
    "            enable_entities(model, labels)\n",
    "            model.update(texts, anns, drop=0.2, losses=losses, sgd=optimizer)\n",
    "            n_docs += len(batch)\n",
    "        meta = {\n",
    "            'docs': n_docs,\n",
    "            'loss': {k: numpy.log(1e-10 + (v / n_docs)) for k,v in losses.items()},\n",
    "        }\n",
    "    enable_all_entities(model)\n",
    "    return meta\n",
    "\n",
    "def train_epoch(model, ds, batch_size, count=None):\n",
    "    if count is None:\n",
    "        dataset = ds.ds_train.copy()\n",
    "        random.shuffle(dataset)\n",
    "    else:\n",
    "        dataset = random.sample(ds.ds_train, count)\n",
    "    batches = minibatch(dataset, size=size_)\n",
    "    _train_epoch(nlp, ds.ents, tqdm_batches(batches, total=len(dataset)))\n",
    "\n",
    "if TESTS or 0:\n",
    "    size_ = compounding(1., 32., 1.001)\n",
    "    train_epoch(nlp, NERUS, batch_size=size_, count=1000)\n",
    "\n",
    "if TESTS or 0:\n",
    "    size_ = compounding(1., 32., 1.001)\n",
    "    train_epoch(nlp, KR, batch_size=size_, count=1000)\n",
    "\n",
    "if TESTS or 0:\n",
    "    res = {}\n",
    "    for c in CORPORA:\n",
    "        res.update(evaluate_data_source(nlp, c, 1000))\n",
    "    display_ents(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     res = {}\n",
    "#     for c in CORPORA:\n",
    "#         res.update(evaluate_data_source(nlp, c, 1000))\n",
    "#     display_ents(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546db2c2d6124a52903c7e47490f8ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8cdabef5a747ccb5355ad969469c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-85d032cc8798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mds_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCORPORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCORPORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-485dea133dfa>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, ds, batch_size, count)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTESTS\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-485dea133dfa>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(model, labels, batches)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0manns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpluck_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'entities'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0menable_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mn_docs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         meta = {\n",
      "\u001b[0;32m/Projects/nlp/spacy-ru/.venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_loss\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Projects/nlp/spacy-ru/.venv/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0m\u001b[1;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "size_ = compounding(1., 32., 1.0005)\n",
    "for e in tqdm(range(60)):\n",
    "    for ds_train in CORPORA:\n",
    "        train_epoch(nlp, ds_train, batch_size=size_, count=1000) # 100000\n",
    "    res = {}\n",
    "    for c in CORPORA:\n",
    "        res.update(evaluate_data_source(nlp, c, 500)) # 5000\n",
    "    display_ents(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training(**CFG)\n",
    "with nlp.use_params(optimizer.averages):\n",
    "    res = evaluate_data_source(nlp, c, count=None)\n",
    "    display_ents(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy.displacy\n",
    "def view_example(nlp, s):\n",
    "    print('Text:', s['raw'])\n",
    "    doc = nlp(s['raw'])\n",
    "#     print(\"Actual:\", [(e, e.label_) for e in doc.ents])\n",
    "    print(\"Expected:\", [(s['raw'][a:b],c,a,b) for a,b,c in s['entities']])\n",
    "    spacy.displacy.render(doc, style='ent')\n",
    "\n",
    "enable_all_entities(nlp)\n",
    "\n",
    "for s in NERUS.ds_test[:2]:\n",
    "    view_example(nlp, s)\n",
    "# for s in KR.ds_test[:3]:\n",
    "#     view_example(nlp, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# save model to output directory\n",
    "def save_model(nlp, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "save_model(nlp, '../../ru2_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer('приветы всем'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(nlp('Вчера мы с Васей пошли гулять по СПб, Гаванская ул. 18'), \n",
    "                      style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
